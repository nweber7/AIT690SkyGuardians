{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce8ae63c",
   "metadata": {},
   "source": [
    "# ISSR Prediction Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197bff94",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02684e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import keras_tuner as kt\n",
    "from joblib import Parallel, delayed\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e201c053",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba5bba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Buffalo_data = pd.read_csv(\"C:/Users/jstej/anaconda3/George Mason DAEN MS/DAEN 690/Buffalo_issr2021.csv\")\n",
    "Buffalo_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783c9639",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Upton_data = pd.read_csv(\"C:/Users/jstej/anaconda3/George Mason DAEN MS/DAEN 690/Upton_issr.csv\")\n",
    "Upton_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0401e0d2",
   "metadata": {},
   "source": [
    "## Filter to 2022 and 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f6a621",
   "metadata": {},
   "outputs": [],
   "source": [
    "Buffalo_data = Buffalo_data[(Buffalo_data['year'] >= 2022) & (Buffalo_data['year'] <= 2023) & (Buffalo_data['press'] != -9999)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a339bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Upton_data = Upton_data[(Upton_data['year'] >= 2022) & (Upton_data['year'] <= 2023) & (Upton_data['press'] != -9999)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec40ff6",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c5a970",
   "metadata": {},
   "source": [
    "### Convert issc from characters to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d83ca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "Buffalo_data['issc'] = Buffalo_data['issc'].replace({'yes': 1, 'no': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ed8a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "Upton_data['issc'] = Upton_data['issc'].replace({'yes': 1, 'no': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126c68c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Buffalo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ed39cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Upton_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d027ca46",
   "metadata": {},
   "source": [
    "## Time Series Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ffb707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create DataFrame with volume column. Volume is based off of the number of times per day that an issc was yes\n",
    "Buffalo_data['date'] = pd.to_datetime(Buffalo_data['date'])\n",
    "issc_counts_per_day_Buffalo = Buffalo_data[Buffalo_data['issc'] == 1].groupby(Buffalo_data['date'].dt.date).size()\n",
    "temp_count_Buffalo = Buffalo_data[Buffalo_data['temp_f'] < -42].groupby(Buffalo_data['date'].dt.date).size()\n",
    "rh_ice_count_Buffalo = Buffalo_data[Buffalo_data['rh_ice'] > 1].groupby(Buffalo_data['date'].dt.date).size()\n",
    "\n",
    "all_dates_Buffalo = pd.DataFrame(index=pd.date_range(start=Buffalo_data['date'].min(), end=Buffalo_data['date'].max(), freq='D'))\n",
    "issc_counts_per_day_Buffalo.name = 'volume Buffalo'\n",
    "temp_count_Buffalo.name = 'temp_volume Buffalo'\n",
    "rh_ice_count_Buffalo.name = 'rhi_volume Buffalo'\n",
    "\n",
    "df2 = all_dates_Buffalo.join(issc_counts_per_day_Buffalo, how='left').fillna(0)\n",
    "\n",
    "df3 = pd.merge(df2, temp_count_Buffalo, left_index = True, right_index = True, how = 'left').fillna(0)\n",
    "\n",
    "new_df_Buffalo = pd.merge(df3, rh_ice_count_Buffalo, left_index = True, right_index = True, how = 'left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4715a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_Buffalo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be730aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create DataFrame with volume column. Volume is based off of the number of times per day that an issc was yes\n",
    "Upton_data['date'] = pd.to_datetime(Upton_data['date'])\n",
    "issc_counts_per_day_Upton = Upton_data[Upton_data['issc'] == 1].groupby(Upton_data['date'].dt.date).size()\n",
    "temp_count_Upton = Upton_data[Upton_data['temp_f'] < -42].groupby(Upton_data['date'].dt.date).size()\n",
    "rh_ice_count_Upton = Upton_data[Upton_data['rh_ice'] > 1].groupby(Upton_data['date'].dt.date).size()\n",
    "\n",
    "all_dates_Upton = pd.DataFrame(index=pd.date_range(start=Upton_data['date'].min(), end=Upton_data['date'].max(), freq='D'))\n",
    "issc_counts_per_day_Upton.name = 'volume Upton'\n",
    "temp_count_Upton.name = 'temp_volume Upton'\n",
    "rh_ice_count_Upton.name = 'rhi_volume Upton'\n",
    "\n",
    "df2 = all_dates_Upton.join(issc_counts_per_day_Upton, how='left').fillna(0)\n",
    "\n",
    "df3 = pd.merge(df2, temp_count_Upton, left_index = True, right_index = True, how = 'left').fillna(0)\n",
    "\n",
    "new_df_Upton = pd.merge(df3, rh_ice_count_Upton, left_index = True, right_index = True, how = 'left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a90171f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_df_Upton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e040b057",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "plt.plot(new_df_Buffalo['volume Buffalo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db30940",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "plt.plot(new_df_Upton['volume Upton'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8c95f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = new_df_Buffalo.merge(new_df_Upton, left_index = True, right_index = True, how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3eac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42d2f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'ISSC in both' attribute\n",
    "df_merge['ISSC in both'] = ((df_merge['volume Buffalo'] > 0) & (df_merge['volume Upton'] > 0)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65de00dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge[df_merge['ISSC in both'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9218f3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = df_merge[df_merge.index != '2023-07-11']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b80226",
   "metadata": {},
   "source": [
    "### Find optimal lag variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e64b70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate optimal number of lag features for ISSC volume, Temp Volume, and RHI Volume\n",
    "issc_lag_B = [0, 1]\n",
    "temp_lag_B = [0, 1]\n",
    "rhi_lag_B = [0, 1]\n",
    "issc_lag_U = [0, 1]\n",
    "temp_lag_U = [0, 1]\n",
    "rhi_lag_U = [0, 1]\n",
    "\n",
    "\n",
    "search_results2 = []\n",
    "\n",
    "def lag_evaluation(issc_lag_B, temp_lag_B, rhi_lag_B, issc_lag_U, temp_lag_U, rhi_lag_U):\n",
    "    lstm_df = df_merge.copy()\n",
    "    \n",
    "    for i in range(1, issc_lag_B + 1):\n",
    "        lstm_df[f'volume Buffalo lag{i}'] = lstm_df['volume Buffalo'].shift(i)\n",
    "        lstm_df[f'volume Buffalo lag{i}'].fillna(0, inplace = True)\n",
    "\n",
    "    for i in range(1, temp_lag_B + 1):\n",
    "        lstm_df[f'temp_volume Buffalo lag{i}'] = lstm_df['temp_volume Buffalo'].shift(i)\n",
    "        lstm_df[f'temp_volume Buffalo lag{i}'].fillna(0, inplace = True)\n",
    "\n",
    "    for i in range(1, rhi_lag_B + 1):\n",
    "        lstm_df[f'rhi_volume Buffalo lag{i}'] = lstm_df['rhi_volume Buffalo'].shift(i)\n",
    "        lstm_df[f'rhi_volume Buffalo lag{i}'].fillna(0, inplace = True)\n",
    "        \n",
    "    for i in range(1, issc_lag_U + 1):\n",
    "        lstm_df[f'volume Upton lag{i}'] = lstm_df['volume Upton'].shift(i)\n",
    "        lstm_df[f'volume Upton lag{i}'].fillna(0, inplace = True)\n",
    "\n",
    "    for i in range(1, temp_lag_U + 1):\n",
    "        lstm_df[f'temp_volume Upton lag{i}'] = lstm_df['temp_volume Upton'].shift(i)\n",
    "        lstm_df[f'temp_volume Upton lag{i}'].fillna(0, inplace = True)\n",
    "\n",
    "    for i in range(1, rhi_lag_U + 1):\n",
    "        lstm_df[f'rhi_volume Upton lag{i}'] = lstm_df['rhi_volume Upton'].shift(i)\n",
    "        lstm_df[f'rhi_volume Upton lag{i}'].fillna(0, inplace = True)\n",
    "            \n",
    "        \n",
    "    X3 = lstm_df.iloc[:, 1:]\n",
    "    y3 = lstm_df['ISSC in both'].values\n",
    "    \n",
    "    X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model = Sequential([\n",
    "        LSTM(units=50, activation='relu', input_shape=(X3_train.shape[1], 1)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='mse')\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X3_train, y3_train, epochs=50, batch_size = 32, verbose=0, validation_split=0.2)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    predictions = model.predict(X3_test)\n",
    "    mse = mean_squared_error(y3_test, predictions)\n",
    "    return mse\n",
    "\n",
    "\n",
    "for vl_B in issc_lag_B:\n",
    "    for tl_B in temp_lag_B:\n",
    "        for rl_B in rhi_lag_B:\n",
    "            for vl_U in issc_lag_U:\n",
    "                for tl_U in temp_lag_U:\n",
    "                    for rl_U in rhi_lag_U:\n",
    "                        mse = lag_evaluation(vl_B, tl_B, rl_B, vl_U, tl_U, rl_U)\n",
    "                        search_results2.append((vl_B, tl_B, rl_B, vl_U, tl_U, rl_U, mse))\n",
    "                        print(f\"ISSC_Lag_B: {vl_B}, Temp_Lag_B: {tl_B}, RHI_Lag_B: {rl_B}, ISSC_Lag:_U {vl_U}, Temp_Lag_U: {tl_U}, RHI_Lag_U: {rl_U}, MSE: {mse}\")\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23efbb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lag = min(search_results2, key=lambda x: x[3])\n",
    "print(f\"Optimal number of lag variables: ISSC Lag B={best_lag[0]}, Temp Lag B={best_lag[1]}, RHI Lag B={best_lag[2]}, ISSC Lag U={best_lag[3]}, Temp Lag U={best_lag[4]}, RHI Lag U={best_lag[5]}, with MSE={best_lag[6]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68ea033",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input the number of lag features\n",
    "num_lags = 1\n",
    "\n",
    "#Create loop to add lag features to dataframe\n",
    "for i in range(1, num_lags + 1):\n",
    "    df_merge[f'temp_volume Upton lag{i}'] = df_merge['temp_volume Upton'].shift(i)\n",
    "    \n",
    "for i in range (1, num_lags + 1):\n",
    "    df_merge[f'rhi_volume Upton lag{i}'] = df_merge['rhi_volume Upton'].shift(i)\n",
    "    \n",
    "for i in range (1, num_lags + 1):\n",
    "    df_merge[f'volume Upton lag{i}'] = df_merge['volume Upton'].shift(i)\n",
    "    \n",
    "\n",
    "for i in range(1, num_lags + 1):\n",
    "    df_merge[f'temp_volume Buffalo lag{i}'] = df_merge['temp_volume Buffalo'].shift(i)\n",
    "    \n",
    "for i in range (1, num_lags + 1):\n",
    "    df_merge[f'rhi_volume Buffalo lag{i}'] = df_merge['rhi_volume Buffalo'].shift(i)\n",
    "    \n",
    "for i in range (1, num_lags + 1):\n",
    "    df_merge[f'volume Buffalo lag{i}'] = df_merge['volume Buffalo'].shift(i)    \n",
    "\n",
    "\n",
    "    \n",
    "# Fill missing values with 0\n",
    "df_merge['temp_volume Upton lag1'] = df_merge['temp_volume Upton lag1'].fillna(0)\n",
    "df_merge['rhi_volume Upton lag1'] = df_merge['rhi_volume Upton lag1'].fillna(0)\n",
    "df_merge['volume Upton lag1'] = df_merge['volume Upton lag1'].fillna(0)\n",
    "\n",
    "df_merge['temp_volume Buffalo lag1'] = df_merge['temp_volume Buffalo lag1'].fillna(0)\n",
    "df_merge['rhi_volume Buffalo lag1'] = df_merge['rhi_volume Buffalo lag1'].fillna(0)\n",
    "df_merge['volume Buffalo lag1'] = df_merge['volume Buffalo lag1'].fillna(0)\n",
    "df_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919f97e5",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bcdb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid of hyperparameters to search\n",
    "units = [50, 100, 150]\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "batch_sizes = [16, 32, 64]\n",
    "\n",
    "# Placeholder for storing the results\n",
    "search_results = []\n",
    "\n",
    "def build_and_evaluate_model(units, learning_rate, batch_size):\n",
    "    # Build the model\n",
    "    X4 = df_merge[['volume Buffalo', 'temp_volume Buffalo', 'rhi_volume Buffalo', 'volume Upton', 'temp_volume Upton', 'rhi_volume Upton']]\n",
    "    y4 = df_merge['ISSC in both'].values\n",
    "\n",
    "    X4_train, X4_test, y4_train, y4_test = train_test_split(X4, y4, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model = Sequential([\n",
    "        LSTM(units=units, activation='relu', input_shape=(X4_train.shape[1], 1)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X4_train, y4_train, epochs=50, batch_size=batch_size, verbose=0, validation_split=0.2)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    predictions = model.predict(X4_test)\n",
    "    mse = mean_squared_error(y4_test, predictions)\n",
    "    return mse\n",
    "\n",
    "# Iterate over each combination of hyperparameters\n",
    "for u in units:\n",
    "    for lr in learning_rates:\n",
    "        for bs in batch_sizes:\n",
    "            mse = build_and_evaluate_model(u, lr, bs)\n",
    "            search_results.append((u, lr, bs, mse))\n",
    "            print(f\"Units: {u}, Learning Rate: {lr}, Batch Size: {bs}, MSE: {mse}\")\n",
    "\n",
    "# Find the best hyperparameters\n",
    "best_hyperparams = min(search_results, key=lambda x: x[3])\n",
    "print(f\"Best Hyperparameters: Units={best_hyperparams[0]}, Learning Rate={best_hyperparams[1]}, Batch Size={best_hyperparams[2]} with MSE={best_hyperparams[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb33c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6627de0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efe8d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X5 = df_merge[['volume Buffalo lag1', 'temp_volume Buffalo lag1', 'rhi_volume Buffalo lag1', 'volume Upton lag1', 'temp_volume Upton lag1', 'rhi_volume Upton lag1']]\n",
    "y5 = df_merge['ISSC in both']\n",
    "\n",
    "X5_train, X5_test, y5_train, y5_test = train_test_split(X5, y5, test_size = 0.2, random_state = 42)\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(units = 150, activation = 'relu', input_shape =(X5.shape[1], 1)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer =Adam(learning_rate = 0.01), loss = 'mse')\n",
    "\n",
    "model.fit(X5_train, y5_train, epochs = 50, batch_size = 32, validation_split = 0.2, verbose = 1)\n",
    "test_loss = model.evaluate(X5_test, y5_test)\n",
    "print(f'Test Loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a382db60",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_volume = df_merge['ISSC in both']\n",
    "predicted_volume = model.predict(X5)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(actual_volume.index, actual_volume, label='Actual Volume', color='blue')\n",
    "plt.plot(actual_volume.index, predicted_volume, label='Predicted Volume', color='red', linestyle='--')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('ISSC both Stations')\n",
    "plt.title('Actual vs. Predicted Volume')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678b7cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
